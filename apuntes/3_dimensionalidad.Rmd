# Reducción de dimensionalidad

## Componentes Principales

El método de Análisis de Componentes Principales se ocupa de explicar la estructura de varianza y covarianza de un grupo de variables a través de unas pocas combinaciones lineales de este grupo de variables. En general sus objetivos son (1) la reducción de los datos y (2) la interpretación.

Algebráicamente, los componentes principales son combinaciones lineales de $p$ variables aleatorias $X_1$, $X_2$, \ldots, $X_p$. Geométricamente, estas combinaciones lineales representan la selección de un nuevo sistema de coordenadas obtenido por rotación de del sistema original con $X_1$, $X_2$, \ldots, $X_p$ como los ejes de coordenadas. Los nuevos ejes representan la dirección con la máxima variabilidad y provee una simple y más parsimoniosa descripción de la estructura de la covarianza.

Los componentes principales dependen únicamente de la matriz de covarianza $\Sigma$ o la matriz de correlaciones $\rho$ (Matriz estandarizada de $\Sigma$) de $X_1$, $X_2$, \ldots, $X_p$. Su desarrollo no requiere de ningún supuesto de normalidad multivariada, sin embargo, componentes principales derivados de poblaciones normales multivariantes tienen un gran uso en la interpretación en términos de elipsoide de densidad constante.

Sea la matriz $\mathbf{X}$ compuesta de $p$ vectores aleatorios $\mathbf{X}=[X_1, X_2, \ldots, X_p ]$ que tiene la matriz de covarianza $\Sigma$ con valores propios $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_p \geq 0$.

Considere la combinación lineal:

\begin{equation}
\begin{array}{rrr}
Y_1		= & a_1^{'} \mathbf{X} = & a_{11} X_1 + a_{12} X_2 + \ldots a_{1p} X_p \\
Y_2		= & a_2^{'} \mathbf{X} = & a_{21} X_1 + a_{22} X_2 + \ldots a_{2p} X_p\\
\vdots	= & \vdots & \vdots \\
Y_p		= & a_p^{'} \mathbf{X} = & a_{p1} X_1 + a_{p2} X_2 + \ldots a_{pp} X_p\\
\end{array}
\label{cp1}
\end{equation}

Equivalente a:

\begin{equation}
\mathbf{Y}= \left[  
\begin{array}{c}
Y_1\\
Y_2\\
\vdots\\
Y_p\\
\end{array}
\right] = \left[  
\begin{array}{cccc}
a_{11} & a_{12} & \ldots & a_{1p} \\
a_{21} & a_{22} & \ldots & a_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
a_{21} & a_{p2} & \ldots & a_{pp} \\
\end{array}
\right] \left[ 
\begin{array}{c}
X_1\\
X_2\\
\vdots\\
X_p\\
\end{array}
\right] = \mathbf{A X}
\label{cp2}
\end{equation}

La combinación lineal $\mathbf{Y}=\mathbf{AX}$ tiene:

\begin{equation}
\mu_y=E(\mathbf{Y})=E(\mathbf{AX})=A \mu_x
\label{cp3}
\end{equation}

\begin{equation}
\Sigma_y=Cov(\mathbf{Y})=Cov(\mathbf{AX})=A \Sigma A^{'}
\label{cp4}
\end{equation}

En base a \ref{cp4}, se obtiene:

\begin{equation}
Var(Y_i)=a_i^{'} \Sigma a_i \quad i=1, 2, \ldots, p
\label{cp5}
\end{equation}

\begin{equation}
Cov(Y_i,Y_k)=a_i^{'} \Sigma a_k \quad i,k=1, 2, \ldots, p
\label{cp6}
\end{equation}

Los componentes principales son combinaciones lineales incorrelacionadas, tal que \ref{cp5} es lo más grande posible.

El primer componente principal es la combinación lineal con máxima varianza. Entonces se debe maximizar $Var(Y_1)=a_1^{'} \Sigma a_1$. Es claro que $Var(Y_1)$ puede ser incrementada multiplicando a $a_1$ por alguna constante. Para eliminar esta indeterminación, es conveniente restringir los coeficientes del vector. Por lo tanto se define.

$$
\begin{array}{rcl}
\text{Primer componente principal} & = & \text{Combinacion lineal} \quad a_1^{'}X \quad \text{que maximiza} \\
								   &   & Var(a_1^{'}X) \quad \text{sujeto a} \quad a_1^{'} a_1=1\\
\text{Segundo componente principal} & = & \text{Combinacion lineal} \quad a_2^{'}X \quad \text{que maximiza} \\
								   &   & Var(a_2^{'}X) \quad \text{sujeto a} \quad a_2^{'} a_2=1 \quad y \\													   &   & Cov(a_1^{'}X,a_2^{'}X)=0			   
\end{array}
$$
 
Para el $i-esimo$ paso:

$$
\begin{array}{rcl}
i-esimo \text{ componente principal} & = & \text{Combinacion lineal} \quad a_i^{'}X \quad \text{que maximiza} \\
								   &   & Var(a_i^{'}X) \quad \text{sujeto a} \quad a_i^{'} a_i=1 \quad y \\													   &   & Cov(a_i^{'}X,a_k^{'}X)=0 \quad	para \quad k<i			   
\end{array}
$$

$$\Sigma_{(pxp)}= A_{(p*p)} \Lambda_{(p*p)} A^t_{(p*p)} $$

El aspecto central de componentes principales es trabajar con menos variables, sea $m<p$, el método de componentes busca a partir de $m$, los siguiente:

$$\Sigma_{(pxp)} \approx  A_{(p*m)} \Lambda_{(m*m)} A^t_{(m*p)}$$

Los pasos sugeridos para iniciar el análisis de componentes principales son:

1. Identificar las variables de interés dentro de la matriz de datos, si las variables tienen las mismas escalas se recomienda emplear la matriz de covarianza, si las escalas son diferentes, se recomienda trabajar con la matriz de correlaciones.
2. Obtener los componentes principales, los eigen valores y la matriz de eigen vectores
3. (Opcional) Eliminar las variables redundantes, 
  * se sugiere identificar las variables del conjunto de datos correlacionadas con los últimos componentes.
4. Calcular nuevamente los componentes principales excluyendo las variables identificadas en el paso previo
5. Elegir el número de componentes a retener $m$ (scree plot, tamaño de los eigen valores, etc)
6. Analizar los resultados y usar los componentes

Usos de componentes principales.

  * Detección de valores atípicos
  * Identificación de posibles factores 
  * Los primeros componentes se pueden usar como indicadores
  * Eliminan la multicolinealidad de un modelo de regresión múltiple

```{r,eval=FALSE}
library(corrplot)
#corrplot(sigma)
knitr::knit_exit()
```

## Análisis de correspondencia

El análisis de correspondencia esta orientado a encontrar relaciones entre las categorías de variables cualitativas. 

Esta técnica es un método visual que va más allá del test de independencia Chi-cuadrado.


```{r,eval=F}
load(url("https://github.com/AlvaroLimber/EST-384/raw/master/data/endsa.RData"))
#tarea 10 min, explorar la base de datos
library(Hmisc)
describe(endsa)
names(endsa)
aux<-attributes(endsa)

names(endsa)
#aeXX
aux$var.labels[7:15]
#nupXX
aux$var.labels[23:26]
#antXX
aux$var.labels[16:20]
#vfXX
aux$var.labels[50:59]
table(endsa$year,endsa$sexo)

table(endsa$edad,endsa$sexo)

library(dplyr)

N<-endsa %>% filter(year==2008) %>% select(ae01,ae08) %>% table()
N<-t1[1:4,]
chisq.test(N)
N
n<-sum(N)
P<-N/n
addmargins(N)
```


Para resumir la teoría, primero divida la matriz de datos $I × J$, denotada por $N$, por su gran total $n$ para obtener la llamada matriz de correspondencia $P = N / n$. Deje que los totales marginales de fila y columna de $P$ sean los vectores $r$ y $c$ respectivamente, es decir, los vectores de masas de fila y columna, y $Dr$ y $Dc$ sean las matrices diagonales de estas matrices. El algoritmo computacional para obtener coordenadas de los perfiles de fila y columna con respecto a los ejes principales, usando el SVD, es el siguiente:

1. Calcular la matriz de residuos estadarizados: $S=D_r^{-1/2}(P-rc^t)D_c^{-1/2}$
2. Calcular la descomposición SVD de $S$: $S=UD_{\alpha}V^t$, donde $U^T U=V^T V=I$
3. Coordenadas principales de filas: $F=D_r^{-1/2} U D_{\alpha}$
4. Coordenadas principales de columnas: $G=D_c^{-1/2} V D_{\alpha}$
5. Coordenadas estándar de filas: $X=D_r^{-1/2} U$
6. Coordenadas estándar de columnas: $Y=D_c^{-1/2} V$
7. Calcular la Inercia: 
$$\phi^2=\sum_i^I\sum_j^J{\frac{(p_{ij}-r_i c_j)^2}{r_i c_j}}$$
8. Graficar las coordenadas de F y G según la la inercia contenida en la matriz $D_{\alpha}$

En R, existe la libreria ca que permite acceder a las coordenadas del método de correspondencia.

```{r,message=F,warning=FALSE,eval=F}
#ejemplo CA
#install.packages("ca")
library(dplyr)
library(ca)
data("smoke")
model<-ca(smoke)
plot(model)
names(model)
summary(model)
#ejemplo ENDSA
load(url("https://github.com/AlvaroLimber/EST-384/raw/master/data/endsa.RData"))
ll<-attributes(endsa)
ll$var.labels
t1<-endsa %>% filter(year==2008) %>% select(7,14) %>% table()
t1<-t1[1:4,]
#test chi2
chisq.test(t1)
model<-ca(t1)
model
plot(model)
#programando el ca
lcol<-colnames(t1)
lrow<-rownames(t1)
P<-prop.table(t1)
r<-margin.table(P,1)
c<-margin.table(P,2)
Dr<-diag(r)
Dc<-diag(c)
##Paso 1
P-r%*%t(c)
#error en las matriz diagonales
Dr^(-0.5)%*%(P-r%*%t(c))%*% Dc^(-0.5)
# corrigiendo el problema
S<-diag(r^(-0.5))%*%(P-r%*%t(c))%*% diag(c^(-0.5))
# 2 descomposición SVD
svd(S)
U<-svd(S)$u
V<-svd(S)$v
Da<-diag(svd(S)$d)
#verificando las propiedades
U %*% t(U) 
t(V) %*% V
U %*% Da %*% t(V)
S
# 3 Coordenadas principales filas
FF<- diag(r^(-0.5)) %*% U %*% Da
# 4 Coordenadas principales columnas
G<- diag(c^(-0.5)) %*% V %*% Da
# 5 Coordenadas estandar filas
X<- diag(r^(-0.5)) %*% U 
# 6 Coordenadas estandar columnas
Y<- diag(c^(-0.5)) %*% V 
# 7 inercia
sum(((P-r%*%t(c))**2)/(r%*%t(c)))
#graficando
xmin<-min(c(FF[,1],G[,1]))
xmax<-max(c(FF[,1],G[,1]))
ymin<-min(c(FF[,2],G[,2]))
ymax<-max(c(FF[,2],G[,2]))
plot(FF[,1],FF[,2],col="red",xlim=c(xmin,xmax)*1.5,ylim=c(ymin,ymax)*1.1)
points(G[,1],G[,2],col="blue")
abline(h=0,v=0,lty=2)
#incluyendo el texto
plot(FF[,1],FF[,2],xlim=c(xmin,xmax)*1.5,ylim=c(ymin,ymax)*1.1,type = "n")
text(FF[,1],FF[,2],labels = lrow,col="red",cex=0.7)
text(G[,1],G[,2],labels = lcol,col="blue",cex=0.7)
abline(h=0,v=0,lty=2)
#incluyendo mas información
plot(FF[,1],FF[,2],xlim=c(xmin,xmax)*1.5,ylim=c(ymin,ymax)*1.1,type = "n")
text(FF[,1],FF[,2],labels = lrow,col="red",cex=0.5+r*2)
text(G[,1],G[,2],labels = lcol,col="blue",cex=0.5+c*2)
abline(h=0,v=0,lty=2)
#viendo solo una dimensión
plot(rep(0,dim(FF)[1]),FF[,1],type = "n", axes = F)
axis(2)
text(rep(0,dim(FF)[1]),FF[,1],labels = lrow,col="red",cex=0.5+r*2)
text(rep(0,dim(G)[1]),G[,1],labels = lcol,col="blue",cex=0.5+c*2)
abline(h=0,v=0,lty=2)
```

### Un ejemplo de correspondencia simple y múltiple

```{r,eval=F}
library(FactoMineR)
library(explor)

endsa08<-endsa %>% filter(year==2008)

model1<-MCA(endsa08[,c("ae01","ae05","ae08","ae09")])
explor(model1)

model1$eig
model1$call

model2<-PCA(endsa08[,c("nup02","nup03","edad")])
explor(model2)
```

